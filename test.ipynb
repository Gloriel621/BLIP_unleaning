{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gloriel621/.conda/envs/blip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_unlearn_loader' from 'data' (/home/gloriel621/BLIP/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warmup_lr_schedule, step_lr_schedule\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_dataset, create_sampler, create_loader, create_unlearn_loader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_unlearn_loader' from 'data' (/home/gloriel621/BLIP/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.blip_pretrain import blip_pretrain\n",
    "import utils\n",
    "from utils import warmup_lr_schedule, step_lr_schedule\n",
    "from data import create_dataset, create_sampler, create_loader, create_forget_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded and saved at: output/Pretrain/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Simulating command line arguments\n",
    "class Args:\n",
    "    config = './configs/unlearn_pretrain.yaml'\n",
    "    output_dir = 'output/Pretrain'\n",
    "    checkpoint = ''\n",
    "    evaluate = False\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    world_size = 1\n",
    "    dist_url = 'env://'\n",
    "    distributed = True\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load configuration\n",
    "with open(args.config, 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.Loader)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration back to a file in the output directory\n",
    "config_path = os.path.join(args.output_dir, 'config.yaml')\n",
    "with open(config_path, 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "\n",
    "print(\"Configuration loaded and saved at:\", config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_file': ['./json_pretrain/coco_updated.json'],\n",
       " 'laion_path': '',\n",
       " 'pretrained': 'output/Pretrain/checkpoint_19.pth',\n",
       " 'vit': 'base',\n",
       " 'vit_grad_ckpt': False,\n",
       " 'vit_ckpt_layer': 0,\n",
       " 'image_size': 224,\n",
       " 'batch_size': 20,\n",
       " 'queue_size': 57600,\n",
       " 'alpha': 0.4,\n",
       " 'weight_decay': 0.05,\n",
       " 'init_lr': 0.0003,\n",
       " 'min_lr': 1e-06,\n",
       " 'warmup_lr': 1e-06,\n",
       " 'lr_decay_rate': 0.9,\n",
       " 'max_epoch': 2,\n",
       " 'warmup_steps': 3000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Creating dataset\n",
      "loading ./json_pretrain/coco_updated.json\n",
      "number of training samples: 566747\n"
     ]
    }
   ],
   "source": [
    "utils.init_distributed_mode(args)    \n",
    "    \n",
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#### Dataset #### \n",
    "#### Dataset #### \n",
    "print(\"Creating dataset\")\n",
    "\n",
    "# create unlearn and retain loaders.\n",
    "dataset = create_dataset('pretrain', config, min_scale=0.2)\n",
    "print('number of training samples: %d'%len(dataset))\n",
    "\n",
    "num_tasks = utils.get_world_size()\n",
    "global_rank = utils.get_rank()            \n",
    "samplers = create_sampler([dataset, dataset], [True, True], num_tasks, global_rank) \n",
    "\n",
    "loaders = create_unlearn_loader(dataset, config['batch_size'], 4, None, samplers, forget_percentage=0.01)\n",
    "retain_loader = loaders['retain']\n",
    "unlearn_loader = loaders['unlearn']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28337"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retain_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlearn_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_indices(dataset, retain_indices)\n",
    "check_indices(dataset, unlearn_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
